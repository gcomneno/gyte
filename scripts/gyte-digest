#!/usr/bin/env bash
set -euo pipefail

# gyte-digest
# - Fetch dei primi N video dal feed YouTube Subscriptions (via cookies browser)
# - Output TSV su stdout + salvataggio in ./in/urls.tsv
# - Colonne: id(001) \t titolo \t url
# - Header commentato con '#'
#
# Uso:
#   gyte-digest [--scan N] [--browser firefox|chrome|chromium|brave|... ] [--out FILE] [--pretty] [--verbose]
#
# Esempi:
#   gyte-digest
#   gyte-digest --scan 12
#   gyte-digest --scan 20 --browser firefox --verbose
#   gyte-digest --out ./in/mia_rassegna.tsv

DEFAULT_SCAN=8
DEFAULT_BROWSER="firefox"
DEFAULT_OUT="./in/urls.tsv"

# Pretty widths (solo per --pretty)
DEFAULT_TITLE_W=70
DEFAULT_URL_W=80

VERBOSE=0
PRETTY=0
SCAN="$DEFAULT_SCAN"
BROWSER="$DEFAULT_BROWSER"
OUTFILE="$DEFAULT_OUT"
TITLE_W="$DEFAULT_TITLE_W"
URL_W="$DEFAULT_URL_W"

usage() {
  cat <<'EOF'
gyte-digest — genera una mini “rassegna” dal feed YouTube Subscriptions.

Output (TSV):
  #id<TAB>title<TAB>url
  001<TAB>...<TAB>https://www.youtube.com/watch?v=...

Opzioni:
  --scan N        Quanti video prendere (default: 8)
  --browser NAME  Browser per cookies-from-browser (default: firefox)
  --out FILE      File TSV da scrivere (default: ./in/urls.tsv)
  --pretty        Stampa una tabella "carina" su stdout (colonne auto + troncamento). Il TSV viene comunque salvato in --out
  --verbose       Abilita log su stderr
  -h, --help      Mostra questo help

Note:
- Richiede yt-dlp.
- Usa cookies dal browser per accedere al feed: https://www.youtube.com/feed/subscriptions
EOF
}

ts() { date "+%Y-%m-%d %H:%M:%S"; }
log() { (( VERBOSE )) && echo "[gyte-digest] $(ts) $*" >&2 || true; }
die() { echo "ERRORE: $*" >&2; exit 1; }

need() { command -v "$1" >/dev/null 2>&1 || die "comando non trovato: $1"; }

is_int() { [[ "${1:-}" =~ ^[0-9]+$ ]]; }

sanitize_scan() {
  local n="$1"
  if ! is_int "$n"; then
    echo "$DEFAULT_SCAN"
    return 0
  fi
  if (( n < 1 )); then
    echo "$DEFAULT_SCAN"
    return 0
  fi
  if (( n > 200 )); then
    echo 200
    return 0
  fi
  echo "$n"
}

# Calcola larghezze pretty in base al terminale (tput cols).
# Layout riga: "%3s | %-*s | %-*s"
# Lunghezza totale = 3 + 3 + WT + 3 + WU = WT + WU + 9
auto_pretty_widths() {
  local cols min_title min_url overhead max_title max_url
  overhead=9
  min_title=20
  min_url=40
  max_title=120
  max_url=160

  cols="$(tput cols 2>/dev/null || true)"
  if ! is_int "${cols:-}"; then
    cols=0
  fi

  # Fallback: terminale sconosciuto → default
  if (( cols < 60 )); then
    TITLE_W="$DEFAULT_TITLE_W"
    URL_W="$DEFAULT_URL_W"
    return 0
  fi

  # Preferisci URL abbastanza largo, ma lascia sempre almeno min_title al titolo.
  # Parto dal default URL e lo adatto a seconda dei cols.
  local url_candidate title_candidate
  url_candidate="$DEFAULT_URL_W"

  # URL non può superare (cols - overhead - min_title)
  local url_max_allowed=$(( cols - overhead - min_title ))
  if (( url_candidate > url_max_allowed )); then
    url_candidate="$url_max_allowed"
  fi

  # URL non può scendere sotto min_url, ma se non ci sta, si sacrifica comunque
  if (( url_candidate < min_url )); then
    url_candidate="$min_url"
  fi

  # Ora ricavo il titolo dallo spazio rimanente
  title_candidate=$(( cols - overhead - url_candidate ))

  # Se il titolo è troppo piccolo, riduco URL finché posso
  if (( title_candidate < min_title )); then
    url_candidate=$(( cols - overhead - min_title ))
    if (( url_candidate < 10 )); then
      url_candidate=10
    fi
    title_candidate=$(( cols - overhead - url_candidate ))
  fi

  # Clamp max
  if (( title_candidate > max_title )); then title_candidate="$max_title"; fi
  if (( url_candidate > max_url )); then url_candidate="$max_url"; fi

  TITLE_W="$title_candidate"
  URL_W="$url_candidate"

  log "pretty widths: cols=$cols title_w=$TITLE_W url_w=$URL_W"
}

pretty_print() {
  # Stampa su stdout una tabella a colonne fisse (senza perdere il TSV salvato su disco).
  # Input: file TSV con header commentato "#id\ttitle\turl".
  local file="$1"
  local w_title="$2"
  local w_url="$3"

  awk -F'\t' -v WT="$w_title" -v WU="$w_url" '
    function trunc(s, w) {
      gsub(/\r$/, "", s);
      gsub(/[[:space:]]+$/, "", s);
      gsub(/^[[:space:]]+/, "", s);
      if (w < 4) return substr(s, 1, w);
      if (length(s) > w) return substr(s, 1, w-3) "...";
      return s;
    }
    function rep(ch, n,   i, s) {
      s="";
      for (i=0; i<n; i++) s = s ch;
      return s;
    }

    BEGIN {
      printf "%3s | %-*s | %-*s\n", "ID", WT, "Titolo", WU, "URL";
      printf "%s-+-%s-+-%s\n", rep("-",3), rep("-",WT), rep("-",WU);
    }

    NR==1 { next }  # salta header TSV (#id ...)
    {
      id=$1; title=$2; url=$3;

      title = trunc(title, WT);
      url   = trunc(url, WU);

      printf "%3s | %-*s | %-*s\n", id, WT, title, WU, url;
    }
  ' "$file"
}

# Parse args
while (( $# > 0 )); do
  case "$1" in
    --scan)
      (( $# >= 2 )) || die "--scan richiede un numero"
      SCAN="$(sanitize_scan "$2")"
      shift 2
      ;;
    --scan=*)
      SCAN="$(sanitize_scan "${1#*=}")"
      shift
      ;;
    --browser)
      (( $# >= 2 )) || die "--browser richiede un argomento"
      BROWSER="$2"
      shift 2
      ;;
    --browser=*)
      BROWSER="${1#*=}"
      shift
      ;;
    --out)
      (( $# >= 2 )) || die "--out richiede un path"
      OUTFILE="$2"
      shift 2
      ;;
    --out=*)
      OUTFILE="${1#*=}"
      shift
      ;;
    --pretty)
      PRETTY=1
      shift
      ;;
    --verbose)
      VERBOSE=1
      shift
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    *)
      die "opzione non riconosciuta: $1 (prova --help)"
      ;;
  esac
done

need yt-dlp
need awk
need sed
need head
need mkdir
need dirname
need mktemp
need tput

mkdir -p "$(dirname "$OUTFILE")"

FEED_URL="https://www.youtube.com/feed/subscriptions"
log "Auto-fetch: browser=$BROWSER scan=$SCAN -> $OUTFILE (TSV: id<TAB>title<TAB>url)"

TMP="$(mktemp -t gyte-digest.XXXXXX.raw)"
CLEAN_TMP="$(mktemp -t gyte-digest.clean.XXXXXX.tsv)"
OUT_TMP="$(mktemp -t gyte-digest.out.XXXXXX.tsv)"
trap 'rm -f "$TMP" "$CLEAN_TMP" "$OUT_TMP"' EXIT

# --- Fetch: title + url
# Nota: yt-dlp può stampare "\t" letterale. Lo normalizziamo dopo.
set +e
yt-dlp \
  --cookies-from-browser "$BROWSER" \
  --js-runtimes deno \
  --extractor-args "youtube:player_client=web_safari" \
  --flat-playlist --skip-download \
  --playlist-items "1-$SCAN" \
  --print "%(title)s\t%(webpage_url)s" \
  "$FEED_URL" \
  >"$TMP" 2>/dev/null
rc=$?
set -e

if (( rc != 0 )); then
  die "yt-dlp non è riuscito a leggere il feed subscriptions (rc=$rc). Prova: --verbose oppure controlla login/cookies del browser."
fi

# --- Normalizzazione e parsing robusto:
# 1) converte \t (due char) in TAB reale
# 2) rimuove CR
# 3) split su TAB
# 4) ripulisce tab dentro al titolo (just in case)
# 5) accetta youtube watch e youtu.be (scarta shorts)
sed -e $'s/\\\\t/\t/g' -e 's/\r$//' "$TMP" \
  | awk -F'\t' '
      BEGIN{OFS="\t"}
      NF>=2 {
        title=$1; url=$2;

        gsub(/\t/," ",title);
        gsub(/[[:space:]]+$/,"",title);
        gsub(/^[[:space:]]+/,"",title);

        if (title=="" || url=="") next;

        # scarta shorts
        if (url ~ /\/shorts\//) next;

        # accetta watch e youtu.be
        if (url ~ /youtube\.com\/watch\?v=/ || url ~ /youtu\.be\//) {
          print title, url;
        }
      }
    ' \
  >"$CLEAN_TMP"

# Header + ID
awk -F'\t' 'BEGIN{OFS="\t"; print "#id","title","url"} {id=sprintf("%03d", ++n); print id,$1,$2}' \
  "$CLEAN_TMP" \
  >"$OUT_TMP"

lines="$(awk 'END{print NR}' "$OUT_TMP")"
if (( lines < 2 )); then
  die "nessun elemento valido trovato nel feed (forse feed vuoto o parsing fallito)."
fi

mv -f "$OUT_TMP" "$OUTFILE"

if (( PRETTY )); then
  auto_pretty_widths
  pretty_print "$OUTFILE" "$TITLE_W" "$URL_W"
else
  cat "$OUTFILE"
fi
